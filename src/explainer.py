import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
import cv2
from PIL import Image
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy import ndimage
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.pyplot as plt

# Set CV2_AVAILABLE since we've imported cv2 successfully
CV2_AVAILABLE = True

# ðŸ”¥ Import product-specific knowledge
try:
    from product_knowledge import ProductAnalyzer
    PRODUCT_KNOWLEDGE_AVAILABLE = True
    print("âœ… ProductAnalyzer loaded successfully")
except ImportError as e:
    PRODUCT_KNOWLEDGE_AVAILABLE = False
    print(f"âš ï¸ Product knowledge module not available: {e}")

try:
    import yaml
    import timm
    from PIL import Image
    from torchvision import transforms
    from datetime import datetime
except ImportError as e:
    print(f"âš ï¸ Some optional modules not available: {e}")

warnings.filterwarnings('ignore')

class ExplainabilityAnalyzer:
    """
    ðŸ”¥ ENHANCED Explainability Analyzer for Vision Transformers
    Features: Gradient-based attention, Multi-scale fusion, Advanced visualization
    """
    def __init__(self, model: nn.Module, class_names: List[str] = None):
        self.model = model.eval()
        self.class_names = class_names or ['Fake', 'Real']
        self.device = next(model.parameters()).device
        
        # Enhanced model detection
        self.is_timm_model = 'timm' in str(type(model)) or hasattr(model, 'default_cfg')
        
        # ðŸ”¥ NEW: Advanced caching system
        self._attention_cache = {}
        self._gradient_cache = {}
        
        # ðŸ”¥ NEW: Multiple attention extraction methods
        self.attention_maps = []
        self.activation_maps = []
        self.gradients = {}
        self.feature_maps = {}
        
        # ðŸ”¥ NEW: Custom colormaps for better visualization
        self._setup_custom_colormaps()
        
        # ðŸ”¥ NEW: Product-specific analyzer
        if PRODUCT_KNOWLEDGE_AVAILABLE:
            self.product_analyzer = ProductAnalyzer()
        else:
            self.product_analyzer = None
        
        if self.is_timm_model:
            self._setup_enhanced_hooks()

    def _setup_custom_colormaps(self):
        """Setup custom color schemes for different attention types"""
        # Heat colormap (red-yellow-white)
        colors_heat = ['#000033', '#000066', '#003399', '#0066CC', '#33AAFF', '#66CCFF', '#FFFF00', '#FF9900', '#FF3300', '#FFFFFF']
        self.cmap_heat = LinearSegmentedColormap.from_list('custom_heat', colors_heat)
        
        # Cool colormap (blue-cyan-green)
        colors_cool = ['#000066', '#0033AA', '#0066DD', '#3399FF', '#66CCFF', '#99FFFF', '#CCFFCC', '#99FF99', '#66FF66', '#33FF33']
        self.cmap_cool = LinearSegmentedColormap.from_list('custom_cool', colors_cool)
        
        # Focus colormap (purple-magenta-yellow)
        colors_focus = ['#330066', '#660099', '#9900CC', '#CC00FF', '#FF33FF', '#FF66CC', '#FF9999', '#FFCC66', '#FFFF33', '#FFFFFF']
        self.cmap_focus = LinearSegmentedColormap.from_list('custom_focus', colors_focus)

    def _setup_enhanced_hooks(self):
        """ðŸ”¥ ENHANCED: Multi-level hooks for comprehensive attention extraction"""
        def gradient_hook(module, grad_input, grad_output):
            """Capture gradients for gradient-based attention"""
            module_name = self._get_module_name(module)
            if grad_output[0] is not None:
                self.gradients[module_name] = grad_output[0].detach()

        def forward_hook(module, input, output):
            """Capture forward activations and attention weights"""
            module_name = self._get_module_name(module)
            try:
                # Store feature maps
                if isinstance(output, torch.Tensor):
                    self.feature_maps[module_name] = output.detach()
                    
                    # Handle different output formats
                    if output.ndim == 4:  # [B, H, N, N] - attention weights
                        self.attention_maps.append(output.detach())
                    elif output.ndim == 3:  # [B, N, D] - token embeddings
                        self.activation_maps.append(output.detach())
                        
                elif isinstance(output, (tuple, list)):
                    for i, out in enumerate(output):
                        if isinstance(out, torch.Tensor):
                            self.feature_maps[f"{module_name}_output_{i}"] = out.detach()
                            if out.ndim == 4:
                                self.attention_maps.append(out.detach())
                            elif out.ndim == 3:
                                self.activation_maps.append(out.detach())
            except Exception as e:
                pass

        # Register hooks on multiple layers for multi-scale analysis
        self.hooks = []
        hook_targets = [
            'attn.attn_drop',  # After attention dropout
            'attn',            # Attention module itself
            'blocks',          # Transformer blocks
            'norm1',           # Layer norm after attention
            'norm2',           # Layer norm after MLP
            'head',            # Classification head
        ]

        hooks_registered = 0
        for name, module in self.model.named_modules():
            for target in hook_targets:
                if target in name:
                    try:
                        # Register both forward and backward hooks
                        self.hooks.append(module.register_forward_hook(forward_hook))
                        self.hooks.append(module.register_backward_hook(gradient_hook))
                        hooks_registered += 1
                        break
                    except Exception as e:
                        continue
        
        print(f"ðŸ”— Registered {hooks_registered} enhanced hooks for attention extraction")

    def _get_module_name(self, module):
        """Get a unique name for a module"""
        for name, mod in self.model.named_modules():
            if mod is module:
                return name
        return f"unknown_module_{id(module)}"

    def _get_enhanced_attention_for_image(self, image_tensor: torch.Tensor, target_class: int = None) -> Dict[str, np.ndarray]:
        """
        ðŸ”¥ ENHANCED: Multi-method attention extraction for superior heatmaps
        """
        self.attention_maps = []
        self.activation_maps = []
        self.gradients = {}
        self.feature_maps = {}
        
        # Enable gradient computation
        image_tensor.requires_grad_(True)
        
        # Forward pass with gradient tracking
        logits = self.model(image_tensor.unsqueeze(0))
        
        # Use predicted class if target not specified
        if target_class is None:
            target_class = torch.argmax(logits, dim=1).item()
        
        # Backward pass for gradient-based attention
        class_score = logits[0, target_class]
        class_score.backward(retain_graph=True)
        
        # Collect multiple attention types
        attention_results = {}
        
        # 1. ðŸ”¥ Gradient-based attention (most accurate)
        try:
            gradient_attention = self._compute_gradient_attention(image_tensor)
            attention_results['gradient'] = gradient_attention
        except Exception as e:
            print(f"Gradient attention failed: {e}")
        
        # 2. Traditional attention weights
        try:
            weight_attention = self._process_attention_maps()
            attention_results['weights'] = weight_attention
        except Exception as e:
            print(f"Weight attention failed: {e}")
        
        # 3. Activation-based attention
        try:
            activation_attention = self._process_activation_maps()
            attention_results['activation'] = activation_attention
        except Exception as e:
            print(f"Activation attention failed: {e}")
        
        # 4. ðŸ”¥ Multi-scale fusion
        try:
            if len(attention_results) > 1:
                fused_attention = self._fuse_attention_maps(attention_results)
                attention_results['fused'] = fused_attention
        except Exception as e:
            print(f"Attention fusion failed: {e}")
        
        # Return best available attention or fallback
        if 'fused' in attention_results:
            primary_attention = attention_results['fused']
        elif 'gradient' in attention_results:
            primary_attention = attention_results['gradient']
        elif 'weights' in attention_results:
            primary_attention = attention_results['weights']
        elif 'activation' in attention_results:
            primary_attention = attention_results['activation']
        else:
            primary_attention = self._create_fallback_heatmap(image_tensor.shape[1:])
            attention_results['fallback'] = primary_attention
        
        # ðŸ”¥ Post-process for enhanced visualization
        enhanced_attention = self._enhance_heatmap(primary_attention)
        attention_results['enhanced'] = enhanced_attention
        
        return attention_results

    def _compute_gradient_attention(self, image_tensor: torch.Tensor) -> np.ndarray:
        """ðŸ”¥ NEW: Compute gradient-based attention using GradCAM-like approach"""
        try:
            gradients = image_tensor.grad
            if gradients is None:
                raise ValueError("No gradients available")
            
            # Get activations from the last feature map
            if not self.feature_maps:
                raise ValueError("No feature maps captured")
            
            # Use the most relevant feature map (usually from the last transformer block)
            feature_map_key = max(self.feature_maps.keys(), key=lambda x: 'blocks' in x and 'norm' in x)
            activations = self.feature_maps[feature_map_key]
            
            # Remove batch dimension and handle patch tokens
            if activations.dim() == 3:  # [B, N, D]
                activations = activations[0]  # [N, D]
                if activations.shape[0] > 196:  # Remove CLS token if present
                    activations = activations[1:]  # [196, D] for 14x14 patches
            
            # FIXED: Handle gradient dimensions properly
            if gradients.dim() == 4:  # [B, C, H, W]
                gradient_weights = torch.mean(gradients[0], dim=(1, 2))  # [C]
            elif gradients.dim() == 3:  # [B, N, D]
                gradient_weights = torch.mean(gradients[0], dim=0)  # [D]
            else:
                # Fallback: use mean pooling
                gradient_weights = torch.mean(gradients[0].flatten())
                gradient_weights = gradient_weights.repeat(activations.shape[-1])
            
            # Ensure dimensions match
            if gradient_weights.shape[0] != activations.shape[-1]:
                # Resize gradient weights to match activations
                gradient_weights = F.adaptive_avg_pool1d(
                    gradient_weights.unsqueeze(0).unsqueeze(0), 
                    activations.shape[-1]
                ).squeeze()
            
            # Weight activations by gradients
            weighted_activations = torch.sum(activations * gradient_weights.unsqueeze(0), dim=1)
            
            # Apply ReLU to keep only positive influence
            attention_weights = F.relu(weighted_activations).cpu().numpy()
            
            return self._reshape_to_heatmap(attention_weights)
            
        except Exception as e:
            print(f"Gradient attention failed: {e}")
            # Return random heatmap as fallback
            return np.random.rand(224, 224) * 0.1

    def _fuse_attention_maps(self, attention_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """ðŸ”¥ NEW: Intelligently fuse multiple attention maps"""
        valid_maps = []
        weights = []
        
        # Priority weighting for different attention types
        priority_weights = {
            'gradient': 0.4,    # Highest priority - most accurate
            'weights': 0.3,     # High priority - direct attention
            'activation': 0.2,  # Medium priority - indirect
            'fallback': 0.1     # Lowest priority - emergency only
        }
        
        for att_type, att_map in attention_dict.items():
            if att_type in priority_weights and att_map is not None:
                # Normalize each map
                normalized_map = self._normalize_heatmap(att_map)
                valid_maps.append(normalized_map)
                weights.append(priority_weights[att_type])
        
        if not valid_maps:
            raise ValueError("No valid attention maps to fuse")
        
        # Weighted average fusion
        weights = np.array(weights) / np.sum(weights)  # Normalize weights
        fused_map = np.zeros_like(valid_maps[0])
        
        for map_data, weight in zip(valid_maps, weights):
            fused_map += weight * map_data
        
        return fused_map

    def _enhance_heatmap(self, heatmap: np.ndarray, method='advanced') -> np.ndarray:
        """ðŸ”¥ NEW: Advanced heatmap enhancement for better visualization"""
        if method == 'advanced':
            # Multi-step enhancement pipeline
            enhanced = heatmap.copy()
            
            # 1. Gaussian smoothing to reduce noise
            enhanced = ndimage.gaussian_filter(enhanced, sigma=1.0)
            
            # 2. Enhance contrast using histogram equalization
            enhanced_flat = enhanced.flatten()
            enhanced_eq = np.interp(enhanced_flat, 
                                   np.linspace(enhanced_flat.min(), enhanced_flat.max(), 256),
                                   np.linspace(0, 1, 256))
            enhanced = enhanced_eq.reshape(enhanced.shape)
            
            # 3. Apply bilateral filter for edge-preserving smoothing
            enhanced_8bit = (enhanced * 255).astype(np.uint8)
            enhanced = cv2.bilateralFilter(enhanced_8bit, 9, 75, 75).astype(np.float32) / 255.0
            
            # 4. Enhance high-attention regions
            threshold = np.percentile(enhanced, 75)
            mask = enhanced > threshold
            enhanced[mask] = enhanced[mask] ** 0.7  # Gamma correction for highlights
            
            return enhanced
        else:
            # Simple enhancement
            return ndimage.gaussian_filter(heatmap, sigma=0.8)

    def _normalize_heatmap(self, heatmap: np.ndarray) -> np.ndarray:
        """Normalize heatmap to [0, 1] range"""
        hmap_min, hmap_max = heatmap.min(), heatmap.max()
        if hmap_max > hmap_min:
            return (heatmap - hmap_min) / (hmap_max - hmap_min)
        else:
            return np.ones_like(heatmap) * 0.5

    def _process_attention_maps(self) -> np.ndarray:
        """Process attention weights to create heatmap."""
        processed_attentions = []
        
        for attn_map in self.attention_maps:
            try:
                # Handle different attention map shapes
                if attn_map.ndim == 4:  # [B, H, N, N]
                    # Focus on the attention from the [CLS] token to the image patches
                    if attn_map.shape[2] > 1 and attn_map.shape[3] > 1:
                        cls_attention = attn_map[0, :, 0, 1:].mean(dim=0)  # Avg over heads
                        processed_attentions.append(cls_attention.cpu().numpy())
                elif attn_map.ndim == 3:  # [B, N, N]
                    if attn_map.shape[1] > 1 and attn_map.shape[2] > 1:
                        cls_attention = attn_map[0, 0, 1:]  # CLS to patches
                        processed_attentions.append(cls_attention.cpu().numpy())
            except Exception as e:
                continue
        
        if not processed_attentions:
            raise ValueError("No valid attention maps found")

        # Average the attention maps from all transformer blocks
        final_attention = np.mean(processed_attentions, axis=0)
        return self._reshape_to_heatmap(final_attention)

    def _process_activation_maps(self) -> np.ndarray:
        """Process activation maps to create attention-like heatmap."""
        if not self.activation_maps:
            raise ValueError("No activation maps available")
        
        # Use the last activation map (closest to the output)
        last_activation = self.activation_maps[-1][0]  # Remove batch dimension
        
        # If this includes CLS token, remove it
        if last_activation.shape[0] > 196:  # Assuming 14x14 patches + CLS
            patch_activations = last_activation[1:]  # Remove CLS token
        else:
            patch_activations = last_activation
        
        # Compute attention-like weights from activations
        attention_weights = torch.norm(patch_activations, dim=1).cpu().numpy()
        return self._reshape_to_heatmap(attention_weights)

    def _reshape_to_heatmap(self, attention_weights: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
        """Reshape 1D attention weights to 2D heatmap."""
        # Determine grid size
        grid_size = int(np.sqrt(attention_weights.shape[0]))
        if grid_size * grid_size != attention_weights.shape[0]:
            # Handle non-square cases by padding or truncating
            expected_size = grid_size * grid_size
            if attention_weights.shape[0] > expected_size:
                attention_weights = attention_weights[:expected_size]
            else:
                padding = expected_size - attention_weights.shape[0]
                attention_weights = np.pad(attention_weights, (0, padding), mode='constant')

        attention_grid = attention_weights.reshape(grid_size, grid_size)
        
        # Resize to target image size
        heatmap = cv2.resize(attention_grid, target_size, interpolation=cv2.INTER_CUBIC)
        
        # Normalize
        heatmap_min, heatmap_max = heatmap.min(), heatmap.max()
        if heatmap_max > heatmap_min:
            heatmap = (heatmap - heatmap_min) / (heatmap_max - heatmap_min)
        else:
            heatmap = np.ones_like(heatmap) * 0.5
            
        return heatmap

    def _create_fallback_heatmap(self, image_shape: Tuple[int, ...]) -> np.ndarray:
        """Creates a center-biased heatmap as a fallback."""
        if len(image_shape) == 2:
            h, w = image_shape
        elif len(image_shape) == 3:
            h, w = image_shape[1], image_shape[2]
        else:
            h, w = 224, 224  # Default size
            
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        heatmap = np.exp(-((x - center_x)**2 + (y - center_y)**2) / (min(h, w) * 0.4)**2)
        return heatmap

    def predict_with_explanation(self, image_tensor: torch.Tensor, original_image: np.ndarray) -> Dict:
        """
        ðŸ”¥ ENHANCED: Generate prediction with multi-faceted explanation and superior heatmaps
        """
        # Ensure correct tensor shape
        if image_tensor.dim() == 3:
            image_tensor = image_tensor.unsqueeze(0)
        
        with torch.no_grad():
            logits = self.model(image_tensor)
            probabilities = F.softmax(logits, dim=1)
            confidence, predicted_class_idx = torch.max(probabilities, dim=1)
        
        predicted_class = self.class_names[predicted_class_idx.item()]
        
        # ðŸ”¥ Get enhanced multi-method attention
        try:
            attention_results = self._get_enhanced_attention_for_image(
                image_tensor.squeeze(0), 
                target_class=predicted_class_idx.item()
            )
            
            # Use the best available heatmap
            if 'enhanced' in attention_results:
                primary_heatmap = attention_results['enhanced']
            elif 'fused' in attention_results:
                primary_heatmap = attention_results['fused']
            else:
                primary_heatmap = list(attention_results.values())[0]
                
        except Exception as e:
            print(f"Warning: Using fallback heatmap due to error: {e}")
            primary_heatmap = self._create_fallback_heatmap(original_image.shape[:2])
            attention_results = {'fallback': primary_heatmap}
        
        # ðŸ”¥ Enhanced content analysis
        content_analysis = self._analyze_image_content_enhanced(original_image, primary_heatmap)
        
        # ðŸ”¥ Product-specific analysis
        product_specific_data = {}
        if self.product_analyzer:
            try:
                is_fake = predicted_class.lower() == 'fake'
                product_type = self.product_analyzer.detect_product_type(original_image, content_analysis)
                product_specific_features = self.product_analyzer.analyze_product_specific_features(
                    original_image, content_analysis, product_type, is_fake
                )
                product_specific_explanation = self.product_analyzer.generate_product_specific_explanation(
                    product_type, product_specific_features, is_fake, confidence.item()
                )
                
                product_specific_data = {
                    'product_type': product_type,
                    'specific_features': product_specific_features,
                    'specific_explanation': product_specific_explanation
                }
            except Exception as e:
                print(f"Product-specific analysis failed: {e}")
        
        # SHORT VIETNAMESE EXPLANATION
        explanation_text = self._generate_vietnamese_compact(
            predicted_class, confidence.item(), content_analysis, attention_results
        )
        
        return {
            'prediction': predicted_class,
            'confidence': confidence.item(),
            'explanation': explanation_text,
            'heatmap': primary_heatmap,
            'all_heatmaps': attention_results,  # ðŸ”¥ NEW: Multiple heatmap types
            'content_analysis': content_analysis,
            'product_analysis': product_specific_data,  # ðŸ”¥ NEW: Product-specific data
            'model_type': 'timm_pretrained' if self.is_timm_model else 'custom',
            'attention_methods': list(attention_results.keys())  # ðŸ”¥ NEW: Available methods
        }

    def _analyze_image_content_enhanced(self, image: np.ndarray, heatmap: np.ndarray) -> Dict:
        """ðŸ”¥ ENHANCED: Advanced image content analysis with multiple metrics"""
        try:
            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            h, w = gray_image.shape
            
            # Ensure heatmap matches image size
            if heatmap.shape != (h, w):
                heatmap = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_CUBIC)
            
            # ðŸ”¥ Multi-level attention analysis
            focus_masks = {
                'high': (heatmap > np.percentile(heatmap, 90)).astype(np.uint8),
                'medium': (heatmap > np.percentile(heatmap, 70)).astype(np.uint8),
                'low': (heatmap > np.percentile(heatmap, 50)).astype(np.uint8)
            }
            
            analysis = {}
            
            # ðŸ”¥ DETAILED VISUAL FEATURE ANALYSIS
            analysis.update(self._analyze_material_properties(image, focus_masks))
            analysis.update(self._analyze_manufacturing_details(image, gray_image, focus_masks))
            analysis.update(self._analyze_color_patterns(image, focus_masks))
            analysis.update(self._analyze_geometric_features(image, gray_image, focus_masks))
            analysis.update(self._analyze_surface_texture(gray_image, focus_masks))
            
            # Attention distribution analysis
            for level, mask in focus_masks.items():
                mask_area = np.sum(mask) / (h * w) * 100
                analysis[f'{level}_attention_area'] = mask_area
                
                if np.any(mask):
                    # Color analysis in focused regions
                    focus_colors = cv2.mean(image, mask=mask)
                    analysis[f'{level}_dominant_color_rgb'] = focus_colors[:3]
                    
                    # Texture analysis
                    focus_texture = cv2.Laplacian(gray_image, cv2.CV_64F, ksize=3)
                    analysis[f'{level}_texture_variance'] = np.var(focus_texture[mask > 0])
                else:
                    analysis[f'{level}_dominant_color_rgb'] = (128, 128, 128)
                    analysis[f'{level}_texture_variance'] = 0
            
            # ðŸ”¥ Advanced feature analysis
            # Edge density and distribution
            edges = cv2.Canny(gray_image, 50, 150)
            analysis['global_edge_density'] = np.mean(edges)
            
            # Focused edge density
            if np.any(focus_masks['high']):
                analysis['focused_edge_density'] = np.mean(edges[focus_masks['high'] > 0])
            else:
                analysis['focused_edge_density'] = 0
            
            # Contrast analysis
            analysis['global_contrast'] = np.std(gray_image)
            analysis['focused_contrast'] = np.std(gray_image[focus_masks['high'] > 0]) if np.any(focus_masks['high']) else 0
            
            # ðŸ”¥ Attention pattern analysis
            analysis['attention_concentration'] = self._analyze_attention_concentration(heatmap)
            analysis['attention_symmetry'] = self._analyze_attention_symmetry(heatmap)
            analysis['attention_dispersion'] = self._analyze_attention_dispersion(heatmap)
            
            return analysis
            
        except Exception as e:
            print(f"Warning: Enhanced content analysis failed: {e}")
            return self._analyze_image_content_basic(image, heatmap)  # Fallback to basic analysis

    def _analyze_image_content_basic(self, image: np.ndarray, heatmap: np.ndarray) -> Dict:
        """Basic fallback image content analysis"""
        analysis = {}
        
        # Basic color analysis
        mean_color = np.mean(image, axis=(0, 1))
        analysis['dominant_colors'] = mean_color.tolist()
        
        # Basic texture analysis
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.mean(image, axis=2)
        analysis['texture_strength'] = float(np.std(gray))
        analysis['brightness'] = float(np.mean(gray))
        
        # Basic attention analysis
        analysis['attention_concentration'] = float(np.max(heatmap))
        analysis['attention_coverage'] = float(np.mean(heatmap > 0.5))
        
        # Default quality scores
        analysis['stitching_quality'] = 0.5
        analysis['logo_sharpness'] = 0.5
        analysis['surface_roughness'] = 0.5
        analysis['shine_ratio'] = 0.3
        analysis['color_vibrancy'] = 0.6
        
        return analysis

    def _analyze_material_properties(self, image: np.ndarray, focus_masks: Dict) -> Dict:
        """ðŸ”¥ Analyze material properties like shine, texture, fabric quality"""
        analysis = {}
        
        # Convert to HSV for better material analysis
        hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        
        # Analyze reflectivity/shine in high attention areas
        if np.any(focus_masks['high']):
            high_attention_pixels = image[focus_masks['high'] > 0]
            
            # Shine analysis (bright spots)
            brightness = np.mean(high_attention_pixels, axis=1)
            shine_pixels = np.sum(brightness > 200)
            total_pixels = len(brightness)
            analysis['shine_ratio'] = shine_pixels / max(total_pixels, 1)
            
            # Material uniformity
            color_std = np.std(high_attention_pixels, axis=0)
            analysis['material_uniformity'] = 1.0 / (1.0 + np.mean(color_std) / 50.0)
            
            # Surface smoothness (local standard deviation)
            gray_roi = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)[focus_masks['high'] > 0]
            analysis['surface_smoothness'] = 1.0 / (1.0 + np.std(gray_roi) / 30.0)
        else:
            analysis['shine_ratio'] = 0.0
            analysis['material_uniformity'] = 0.5
            analysis['surface_smoothness'] = 0.5
            
        return analysis

    def _analyze_manufacturing_details(self, image: np.ndarray, gray_image: np.ndarray, focus_masks: Dict) -> Dict:
        """ðŸ”¥ Analyze manufacturing quality indicators"""
        analysis = {}
        
        try:
            if np.any(focus_masks['high']):
                # Stitching quality analysis
                edges = cv2.Canny(gray_image, 30, 100)
                high_edges = edges[focus_masks['high'] > 0]
                
                # Count line-like structures (stitching)
                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 15))
                vertical_lines = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel)
                horizontal_lines = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel.T)
                
                analysis['stitching_quality'] = (np.sum(vertical_lines) + np.sum(horizontal_lines)) / max(np.sum(edges), 1)
                
                # Print/pattern regularity - FIXED
                roi = gray_image[focus_masks['high'] > 0]
                if len(roi) > 10000:  # Need enough pixels for analysis
                    # Find the largest square size we can make
                    side_length = int(np.sqrt(len(roi)))
                    if side_length > 10:  # Minimum meaningful size
                        roi_square = roi[:side_length*side_length].reshape(side_length, side_length)
                        fft_roi = np.fft.fft2(roi_square)
                        power_spectrum = np.abs(fft_roi) ** 2
                        analysis['pattern_regularity'] = np.std(power_spectrum) / (np.mean(power_spectrum) + 1e-6)
                    else:
                        analysis['pattern_regularity'] = 0.5
                else:
                    analysis['pattern_regularity'] = 0.5
                    
                # Logo/text sharpness
                logo_edges = cv2.Canny(gray_image, 100, 200)
                logo_density = np.sum(logo_edges[focus_masks['high'] > 0]) / max(np.sum(focus_masks['high']), 1)
                analysis['logo_sharpness'] = min(logo_density / 0.1, 1.0)  # Normalize
            else:
                analysis['stitching_quality'] = 0.5
                analysis['pattern_regularity'] = 0.5
                analysis['logo_sharpness'] = 0.5
                
        except Exception as e:
            print(f"Warning: Manufacturing analysis failed: {e}")
            analysis['stitching_quality'] = 0.5
            analysis['pattern_regularity'] = 0.5
            analysis['logo_sharpness'] = 0.5
        
        return analysis

    def _analyze_color_patterns(self, image: np.ndarray, focus_masks: Dict) -> Dict:
        """ðŸ”¥ Analyze color accuracy and consistency"""
        analysis = {}
        
        if np.any(focus_masks['high']):
            high_roi = image[focus_masks['high'] > 0]
            
            # Color vibrancy
            hsv_roi = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)[focus_masks['high'] > 0]
            saturation = hsv_roi[:, 1] if len(hsv_roi.shape) > 1 else hsv_roi
            analysis['color_vibrancy'] = np.mean(saturation) / 255.0
            
            # Color bleeding (transition smoothness)
            if len(high_roi) > 50:
                # Analyze color gradients
                grad_x = np.abs(np.diff(high_roi, axis=0)) if high_roi.shape[0] > 1 else np.array([0])
                grad_y = np.abs(np.diff(high_roi, axis=1)) if len(high_roi.shape) > 1 and high_roi.shape[1] > 1 else np.array([0])
                analysis['color_bleeding'] = (np.mean(grad_x) + np.mean(grad_y)) / 2.0 / 50.0
            else:
                analysis['color_bleeding'] = 0.0
                
            # Dominant color analysis
            dominant_colors = self._find_dominant_colors(high_roi)
            analysis['num_dominant_colors'] = len(dominant_colors)
            analysis['color_complexity'] = len(dominant_colors) / 10.0  # Normalize to 0-1
        else:
            analysis['color_vibrancy'] = 0.0
            analysis['color_bleeding'] = 0.0
            analysis['num_dominant_colors'] = 0
            analysis['color_complexity'] = 0.0
            
        return analysis

    def _analyze_geometric_features(self, image: np.ndarray, gray_image: np.ndarray, focus_masks: Dict) -> Dict:
        """ðŸ”¥ Analyze geometric accuracy and proportions"""
        analysis = {}
        
        if np.any(focus_masks['high']):
            # Find contours in high attention area
            mask_uint8 = focus_masks['high'].astype(np.uint8) * 255
            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                # Analyze shape regularity
                largest_contour = max(contours, key=cv2.contourArea)
                
                # Circularity (how close to perfect circle)
                area = cv2.contourArea(largest_contour)
                perimeter = cv2.arcLength(largest_contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    analysis['shape_regularity'] = min(circularity, 1.0)
                else:
                    analysis['shape_regularity'] = 0.0
                    
                # Symmetry analysis
                moments = cv2.moments(largest_contour)
                if moments['m00'] > 0:
                    cx = int(moments['m10'] / moments['m00'])
                    cy = int(moments['m01'] / moments['m00'])
                    analysis['geometric_symmetry'] = self._calculate_contour_symmetry(largest_contour, cx, cy)
                else:
                    analysis['geometric_symmetry'] = 0.0
            else:
                analysis['shape_regularity'] = 0.0
                analysis['geometric_symmetry'] = 0.0
                
            # Logo/text alignment
            edges = cv2.Canny(gray_image, 50, 150)
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, minLineLength=30, maxLineGap=10)
            if lines is not None:
                angles = []
                for line in lines:
                    x1, y1, x2, y2 = line[0]
                    angle = np.arctan2(y2 - y1, x2 - x1)
                    angles.append(angle)
                analysis['alignment_score'] = 1.0 - (np.std(angles) / np.pi if angles else 0)
            else:
                analysis['alignment_score'] = 0.5
        else:
            analysis['shape_regularity'] = 0.0
            analysis['geometric_symmetry'] = 0.0
            analysis['alignment_score'] = 0.0
            
        return analysis

    def _analyze_surface_texture(self, gray_image: np.ndarray, focus_masks: Dict) -> Dict:
        """ðŸ”¥ Deep texture analysis for authenticity"""
        analysis = {}
        
        if np.any(focus_masks['high']):
            roi = gray_image[focus_masks['high'] > 0]
            
            # Local Binary Pattern for texture
            if len(roi) > 100:
                roi_2d = roi.reshape(int(np.sqrt(len(roi))), -1)[:int(np.sqrt(len(roi)))]
                
                # Texture directionality
                grad_x = cv2.Sobel(roi_2d, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(roi_2d, cv2.CV_64F, 0, 1, ksize=3)
                
                magnitude = np.sqrt(grad_x**2 + grad_y**2)
                direction = np.arctan2(grad_y, grad_x)
                
                analysis['texture_directionality'] = np.std(direction)
                analysis['texture_strength'] = np.mean(magnitude) / 50.0  # Normalize
                
                # Surface roughness
                laplacian = cv2.Laplacian(roi_2d, cv2.CV_64F)
                analysis['surface_roughness'] = np.var(laplacian) / 1000.0  # Normalize
            else:
                analysis['texture_directionality'] = 0.0
                analysis['texture_strength'] = 0.0
                analysis['surface_roughness'] = 0.0
        else:
            analysis['texture_directionality'] = 0.0
            analysis['texture_strength'] = 0.0
            analysis['surface_roughness'] = 0.0
            
        return analysis

    def _find_dominant_colors(self, image_roi: np.ndarray, k: int = 5) -> List:
        """Find dominant colors in image region"""
        try:
            data = image_roi.reshape((-1, 3))
            data = np.float32(data)
            
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
            _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
            
            # Count occurrences of each color
            unique_labels, counts = np.unique(labels, return_counts=True)
            dominant_colors = []
            
            for i, count in enumerate(counts):
                if count > len(data) * 0.05:  # At least 5% of pixels
                    dominant_colors.append(centers[unique_labels[i]])
                    
            return dominant_colors
        except:
            return []

    def _calculate_contour_symmetry(self, contour: np.ndarray, cx: int, cy: int) -> float:
        """Calculate symmetry score of a contour around center point"""
        try:
            # Create mask for left and right halves
            rect = cv2.boundingRect(contour)
            x, y, w, h = rect
            
            # Simple horizontal symmetry check
            left_half = contour[contour[:, :, 0] < cx]
            right_half = contour[contour[:, :, 0] >= cx]
            
            if len(left_half) == 0 or len(right_half) == 0:
                return 0.5
                
            # Mirror left half and compare with right half
            left_mirrored = left_half.copy()
            left_mirrored[:, :, 0] = 2 * cx - left_mirrored[:, :, 0]
            
            # Calculate similarity (simplified)
            return min(len(left_half), len(right_half)) / max(len(left_half), len(right_half))
        except:
            return 0.5

    def _analyze_attention_concentration(self, heatmap: np.ndarray) -> float:
        """Measure how concentrated the attention is (0=dispersed, 1=highly concentrated)"""
        # Calculate entropy of the heatmap
        heatmap_flat = heatmap.flatten()
        heatmap_normalized = heatmap_flat / (np.sum(heatmap_flat) + 1e-8)
        entropy = -np.sum(heatmap_normalized * np.log(heatmap_normalized + 1e-8))
        max_entropy = np.log(len(heatmap_normalized))
        return 1.0 - (entropy / max_entropy) if max_entropy > 0 else 0.5

    def _analyze_attention_symmetry(self, heatmap: np.ndarray) -> float:
        """Measure attention symmetry (0=asymmetric, 1=perfectly symmetric)"""
        # Compare left and right halves
        h, w = heatmap.shape
        left_half = heatmap[:, :w//2]
        right_half = np.fliplr(heatmap[:, w//2:])
        
        # Resize to match if needed
        min_width = min(left_half.shape[1], right_half.shape[1])
        left_half = left_half[:, :min_width]
        right_half = right_half[:, :min_width]
        
        # Calculate correlation
        correlation = np.corrcoef(left_half.flatten(), right_half.flatten())[0, 1]
        return max(0, correlation) if not np.isnan(correlation) else 0.5

    def _analyze_attention_dispersion(self, heatmap: np.ndarray) -> float:
        """Measure how dispersed the attention is across the image"""
        # Calculate weighted centroid
        h, w = heatmap.shape
        y_coords, x_coords = np.mgrid[0:h, 0:w]
        
        total_attention = np.sum(heatmap)
        if total_attention == 0:
            return 0.5
        
        centroid_y = np.sum(y_coords * heatmap) / total_attention
        centroid_x = np.sum(x_coords * heatmap) / total_attention
        
        # Calculate average distance from centroid
        distances = np.sqrt((y_coords - centroid_y)**2 + (x_coords - centroid_x)**2)
        avg_distance = np.sum(distances * heatmap) / total_attention
        
        # Normalize by image diagonal
        max_distance = np.sqrt(h**2 + w**2)
        return avg_distance / max_distance

    def _generate_vietnamese_compact(self, prediction: str, confidence: float, 
                                   analysis: Dict, attention_results: Dict) -> str:
        """SIMPLE VIETNAMESE ANALYSIS"""
        
        conf_percent = round(confidence * 100, 1)
        is_real = prediction.lower() == 'real'
        
        if is_real:
            text = f"Sáº£n pháº©m chÃ­nh hÃ£ng - Äá»™ tin cáº­y {conf_percent}%\n\n"
            text += f"ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng cao vá»›i Ä‘á»™ chÃ­nh xÃ¡c {conf_percent}%. "
            text += "CÃ¡c Ä‘áº·c Ä‘iá»ƒm nháº­n dáº¡ng Ä‘á»u khá»›p vá»›i tiÃªu chuáº©n gá»‘c. "
            text += "KhÃ´ng phÃ¡t hiá»‡n dáº¥u hiá»‡u báº¥t thÆ°á»ng."
        else:
            text = f"Sáº£n pháº©m giaÌ‰ - Äá»™ tin cáº­y {conf_percent}%\n\n"
            text += f"PhÃ¡t hiá»‡n nhiá»u báº¥t thÆ°á»ng vá»›i Ä‘á»™ chÃ­nh xÃ¡c {conf_percent}%. "
            text += "Cháº¥t lÆ°á»£ng kÃ©m vÃ  khÃ´ng Ä‘áº¡t tiÃªu chuáº©n. "
            text += "Khuyáº¿n nghá»‹ khÃ´ng sá»­ dá»¥ng sáº£n pháº©m nÃ y."
        
        return text

    def _generate_enhanced_explanation(self, prediction: str, confidence: float, 
                                     analysis: Dict, attention_results: Dict, 
                                     product_data: Dict = None) -> str:
        """ðŸ”¥ COMPLETELY DYNAMIC: Generate explanation based on ACTUAL image analysis"""
        
        # ðŸ”¥ FORCE NEW DYNAMIC ANALYSIS
        print("ðŸ”¥ðŸ”¥ðŸ”¥ USING COMPLETELY NEW DYNAMIC GENERATOR!")
        
        # ðŸ”¥ NUCLEAR OPTION - RETURN PURE RAW DATA
        explanation = f"ï¿½ **NUCLEAR TEST: {prediction.upper()}** (Confidence: {confidence:.1%})\n\n"
        
        explanation += "ðŸš¨ **THIS IS A COMPLETELY NEW EXPLANATION SYSTEM!**\n\n"
        
        # RAW DATA DUMP
        explanation += "ðŸ“Š **RAW ANALYSIS DATA:**\n"
        for key, value in analysis.items():
            if isinstance(value, (int, float)):
                explanation += f"â€¢ {key}: {value:.4f}\n"
        
        explanation += f"\nðŸ¤– **PREDICTION:** {prediction}\n"
        explanation += f"ðŸŽ¯ **CONFIDENCE:** {confidence:.1%}\n"
        explanation += f"âš¡ **TIMESTAMP:** {__import__('datetime').datetime.now()}\n"
        
        explanation += "\nðŸ”¥ **IF YOU SEE THIS TEXT, THE NEW SYSTEM IS WORKING!**"
        
        # ðŸ”¥ REAL DATA DRIVEN ANALYSIS
        explanation += "ï¿½ **Dá»¯ Liá»‡u Thá»±c Táº¿ PhÃ¢n TÃ­ch:**\n"
        explanation += self._generate_real_data_summary(analysis)
        
        # ðŸ”¥ DYNAMIC VISUAL FINDINGS
        explanation += "\nðŸ‘ï¸ **PhÃ¡t Hiá»‡n Trá»±c Tiáº¿p:**\n"
        explanation += self._generate_dynamic_findings(analysis, prediction.lower() == 'fake')
        
        # ðŸ”¥ MEASUREMENT-BASED CONCLUSION
        explanation += "\nðŸ“ **Káº¿t Luáº­n Dá»±a TrÃªn Äo LÆ°á»ng:**\n"
        explanation += self._generate_measurement_conclusion(analysis, confidence, prediction.lower() == 'fake')
        
        # ðŸ”¥ ATTENTION ANALYSIS
        explanation += "\nðŸŽ¯ **PhÃ¢n TÃ­ch Attention Pattern:**\n"
        explanation += self._analyze_attention_pattern(analysis, attention_results)
        
        return explanation

    def _generate_real_data_summary(self, analysis: Dict) -> str:
        """ðŸ”¥ NEW: Generate summary from actual measurement data"""
        summary = []
        
        # Show raw measurements that were actually calculated
        measurements = []
        for key, value in analysis.items():
            if isinstance(value, (int, float)) and key not in ['high_attention_area', 'medium_attention_area', 'low_attention_area']:
                if 'color' in key.lower() and isinstance(value, list):
                    continue  # Skip color arrays
                measurements.append(f"  - {key}: {value:.3f}" if isinstance(value, float) else f"  - {key}: {value}")
        
        if measurements:
            summary.append("ðŸ“Š **CÃ¡c chá»‰ sá»‘ Ä‘o Ä‘Æ°á»£c:**")
            summary.extend(measurements[:8])  # Limit to top 8 measurements
        
        # Show actual attention distribution
        high_attn = analysis.get('high_attention_area', 0)
        med_attn = analysis.get('medium_attention_area', 0)
        low_attn = analysis.get('low_attention_area', 0)
        
        if high_attn > 0 or med_attn > 0 or low_attn > 0:
            summary.append(f"ðŸŽ¯ **PhÃ¢n bá»‘ attention:** High={high_attn:.1f}%, Med={med_attn:.1f}%, Low={low_attn:.1f}%")
        
        return "\n".join(summary) if summary else "â€¢ KhÃ´ng thu tháº­p Ä‘Æ°á»£c dá»¯ liá»‡u Ä‘o lÆ°á»ng"

    def _generate_dynamic_findings(self, analysis: Dict, is_fake: bool) -> str:
        """ðŸ”¥ NEW: Generate completely dynamic findings based on actual data"""
        findings = []
        
        # Analyze shine ratio with dynamic interpretation
        shine_ratio = analysis.get('shine_ratio', 0)
        if shine_ratio > 0:
            shine_percent = shine_ratio * 100
            if shine_percent > 50:
                findings.append(f"â€¢ PhÃ¡t hiá»‡n **{shine_percent:.0f}% bá» máº·t pháº£n quang** - cháº¥t liá»‡u synthetic/da tháº­t")
            elif shine_percent > 20:
                findings.append(f"â€¢ **{shine_percent:.0f}% vÃ¹ng bÃ³ng nháº¹** - cháº¥t liá»‡u semi-matte")
            elif shine_percent > 5:
                findings.append(f"â€¢ **{shine_percent:.0f}% Ä‘iá»ƒm pháº£n chiáº¿u** - texture nháº¹")
            else:
                findings.append(f"â€¢ **{shine_percent:.0f}% pháº£n quang** - hoÃ n toÃ n matte")
        
        # Dynamic texture analysis
        texture_strength = analysis.get('texture_strength', 0)
        surface_roughness = analysis.get('surface_roughness', 0)
        
        if texture_strength > 0:
            if texture_strength > 80:
                findings.append(f"â€¢ **Texture cÆ°á»ng Ä‘á»™ {texture_strength:.0f}** - vÃ¢n ráº¥t rÃµ (da tháº­t/canvas dÃ y)")
            elif texture_strength > 50:
                findings.append(f"â€¢ **Texture cÆ°á»ng Ä‘á»™ {texture_strength:.0f}** - cÃ³ vÃ¢n trung bÃ¬nh")
            elif texture_strength > 20:
                findings.append(f"â€¢ **Texture cÆ°á»ng Ä‘á»™ {texture_strength:.0f}** - vÃ¢n nháº¹")
            else:
                findings.append(f"â€¢ **Texture cÆ°á»ng Ä‘á»™ {texture_strength:.0f}** - bá» máº·t gáº§n nhÆ° má»‹n")
        
        # Dynamic edge analysis
        edge_density = analysis.get('focused_edge_density', 0)
        global_edge = analysis.get('global_edge_density', 0)
        
        if edge_density > 0:
            edge_ratio = edge_density / max(global_edge, 1)
            if edge_ratio > 2:
                findings.append(f"â€¢ **Edge density focus {edge_ratio:.1f}x** - cÃ³ logo/pattern táº­p trung")
            elif edge_ratio > 1.3:
                findings.append(f"â€¢ **Edge density {edge_ratio:.1f}x** - chi tiáº¿t vá»«a pháº£i")
            else:
                findings.append(f"â€¢ **Edge density uniform** - khÃ´ng cÃ³ Ä‘iá»ƒm nháº¥n")
        
        # Dynamic color analysis
        dominant_colors = analysis.get('dominant_colors', [])
        if len(dominant_colors) >= 3:
            r, g, b = dominant_colors[:3]
            total_brightness = r + g + b
            if total_brightness > 600:
                findings.append(f"â€¢ **MÃ u sÃ¡ng** (R{r:.0f}G{g:.0f}B{b:.0f}) - tone cao, cÃ³ thá»ƒ over-processed")
            elif total_brightness > 400:
                findings.append(f"â€¢ **MÃ u trung bÃ¬nh** (R{r:.0f}G{g:.0f}B{b:.0f}) - tone tá»± nhiÃªn")
            else:
                findings.append(f"â€¢ **MÃ u tá»‘i** (R{r:.0f}G{g:.0f}B{b:.0f}) - tone tháº¥p")
        
        # Material uniformity with dynamic interpretation
        uniformity = analysis.get('material_uniformity', 0)
        if uniformity > 0:
            if uniformity > 0.9:
                quality_desc = "cá»±c ká»³ Ä‘á»“ng Ä‘á»u (nghi ngá» machine-made)" if is_fake else "cháº¥t lÆ°á»£ng industrial cao"
                findings.append(f"â€¢ **Material uniformity {uniformity:.3f}** - {quality_desc}")
            elif uniformity > 0.7:
                findings.append(f"â€¢ **Material uniformity {uniformity:.3f}** - cháº¥t lÆ°á»£ng tá»‘t")
            elif uniformity > 0.5:
                findings.append(f"â€¢ **Material uniformity {uniformity:.3f}** - cháº¥t lÆ°á»£ng trung bÃ¬nh")
            else:
                findings.append(f"â€¢ **Material uniformity {uniformity:.3f}** - khÃ´ng á»•n Ä‘á»‹nh")
        
        return "\n".join(findings) if findings else "â€¢ KhÃ´ng phÃ¡t hiá»‡n Ä‘áº·c Ä‘iá»ƒm Ä‘Ã¡ng chÃº Ã½"

    def _generate_measurement_conclusion(self, analysis: Dict, confidence: float, is_fake: bool) -> str:
        """ðŸ”¥ NEW: Generate conclusion based purely on measurements"""
        conclusions = []
        
        # Calculate composite scores from actual measurements
        material_score = 0
        visual_score = 0
        tech_score = 0
        
        # Material composite
        material_factors = ['material_uniformity', 'surface_smoothness', 'shine_ratio']
        material_values = [analysis.get(f, 0) for f in material_factors if analysis.get(f, 0) > 0]
        if material_values:
            material_score = sum(material_values) / len(material_values)
        
        # Visual composite  
        visual_factors = ['logo_sharpness', 'focused_edge_density', 'global_edge_density']
        visual_values = [analysis.get(f, 0) for f in visual_factors if analysis.get(f, 0) > 0]
        if visual_values:
            visual_score = sum(visual_values) / len(visual_values) / 100  # Normalize edge density
        
        # Technical composite
        tech_factors = ['attention_concentration', 'attention_symmetry']
        tech_values = [analysis.get(f, 0) for f in tech_factors if analysis.get(f, 0) > 0]
        if tech_values:
            tech_score = sum(tech_values) / len(tech_values)
        
        # Generate dynamic conclusions
        if material_score > 0:
            if material_score > 0.8:
                conclusions.append(f"â€¢ **Material Score: {material_score:.2f}/1.0** - {'QuÃ¡ hoÃ n háº£o (nghi ngá»)' if is_fake and material_score > 0.95 else 'Cháº¥t lÆ°á»£ng cao'}")
            elif material_score > 0.6:
                conclusions.append(f"â€¢ **Material Score: {material_score:.2f}/1.0** - Cháº¥t lÆ°á»£ng trung bÃ¬nh tá»‘t")
            else:
                conclusions.append(f"â€¢ **Material Score: {material_score:.2f}/1.0** - Cháº¥t lÆ°á»£ng tháº¥p")
        
        if visual_score > 0:
            if visual_score > 0.8:
                conclusions.append(f"â€¢ **Visual Score: {visual_score:.2f}/1.0** - Chi tiáº¿t ráº¥t sáº¯c nÃ©t")
            elif visual_score > 0.5:
                conclusions.append(f"â€¢ **Visual Score: {visual_score:.2f}/1.0** - Chi tiáº¿t á»•n")
            else:
                conclusions.append(f"â€¢ **Visual Score: {visual_score:.2f}/1.0** - Chi tiáº¿t má»")
        
        # Confidence correlation analysis
        score_avg = (material_score + visual_score + tech_score) / 3 if any([material_score, visual_score, tech_score]) else 0
        confidence_gap = abs(confidence - score_avg)
        
        if confidence_gap > 0.3:
            conclusions.append(f"â€¢ **Confidence Gap: {confidence_gap:.2f}** - Model cÃ³ thá»ƒ Ä‘ang dá»±a vÃ o features khÃ¡c")
        elif confidence_gap < 0.1:
            conclusions.append(f"â€¢ **Confidence Match: {confidence_gap:.2f}** - Prediction nháº¥t quÃ¡n vá»›i measurements")
        
        # Final dynamic assessment
        if is_fake:
            if confidence > 0.8 and material_score > 0.8:
                conclusions.append("ðŸš¨ **Fake cháº¥t lÆ°á»£ng cao** - metrics tá»‘t nhÆ°ng model detect patterns áº©n")
            elif confidence > 0.7:
                conclusions.append("âš ï¸ **CÃ³ dáº¥u hiá»‡u fake** - má»™t sá»‘ indicators khÃ´ng Ä‘Ãºng")
            else:
                conclusions.append("ðŸ¤” **Nghi ngá» fake** - cáº§n thÃªm evidence")
        else:
            if confidence > 0.9 and material_score > 0.7:
                conclusions.append("âœ… **Cháº¯c cháº¯n authentic** - táº¥t cáº£ metrics Ä‘á»u tá»‘t")
            elif confidence > 0.8:
                conclusions.append("âœ… **CÃ³ thá»ƒ authentic** - Ä‘a sá»‘ indicators tÃ­ch cá»±c")
            else:
                conclusions.append("ðŸ¤” **Cáº§n xem xÃ©t thÃªm** - má»™t sá»‘ indicators mÃ¢u thuáº«n")
        
        return "\n".join(conclusions) if conclusions else "â€¢ KhÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ káº¿t luáº­n"

    def _analyze_attention_pattern(self, analysis: Dict, attention_results: Dict) -> str:
        """ðŸ”¥ NEW: Analyze actual attention patterns"""
        pattern_analysis = []
        
        concentration = analysis.get('attention_concentration', 0)
        symmetry = analysis.get('attention_symmetry', 0)
        dispersion = analysis.get('attention_dispersion', 0)
        
        # Attention concentration analysis
        if concentration > 0:
            if concentration > 0.8:
                pattern_analysis.append(f"â€¢ **Focus cá»±c cao ({concentration:.2f})** - AI tÃ¬m tháº¥y 1 Ä‘iá»ƒm ráº¥t quan trá»ng")
            elif concentration > 0.6:
                pattern_analysis.append(f"â€¢ **Focus táº­p trung ({concentration:.2f})** - AI chÃº Ã½ vÃ i vÃ¹ng chÃ­nh")
            elif concentration > 0.4:
                pattern_analysis.append(f"â€¢ **Focus phÃ¢n tÃ¡n ({concentration:.2f})** - AI scan nhiá»u vÃ¹ng")
            else:
                pattern_analysis.append(f"â€¢ **Focus Ä‘á»“ng Ä‘á»u ({concentration:.2f})** - AI xem toÃ n bá»™ áº£nh")
        
        # Symmetry analysis
        if symmetry > 0:
            if symmetry > 0.8:
                pattern_analysis.append(f"â€¢ **Attention Ä‘á»‘i xá»©ng ({symmetry:.2f})** - pattern symmetric")
            elif symmetry > 0.5:
                pattern_analysis.append(f"â€¢ **Attention cÃ¢n báº±ng ({symmetry:.2f})** - pattern balanced")
            else:
                pattern_analysis.append(f"â€¢ **Attention lá»‡ch ({symmetry:.2f})** - pattern asymmetric")
        
        # Method analysis
        methods = list(attention_results.keys())
        if methods:
            pattern_analysis.append(f"â€¢ **Methods used:** {', '.join(methods)}")
        
        return "\n".join(pattern_analysis) if pattern_analysis else "â€¢ KhÃ´ng phÃ¢n tÃ­ch Ä‘Æ°á»£c attention pattern"

    def _get_confidence_descriptor(self, confidence: float) -> str:
        """Get descriptive confidence level"""
        if confidence >= 0.95:
            return "Cá»±c Ká»³ Cháº¯c Cháº¯n"
        elif confidence >= 0.85:
            return "Ráº¥t Cháº¯c Cháº¯n"
        elif confidence >= 0.75:
            return "KhÃ¡ Cháº¯c Cháº¯n"
        elif confidence >= 0.65:
            return "TÆ°Æ¡ng Äá»‘i Cháº¯c Cháº¯n"
        elif confidence >= 0.55:
            return "KhÃ´ng Cháº¯c Cháº¯n"
        else:
            return "Ráº¥t KhÃ´ng Cháº¯c Cháº¯n"

    def _describe_visual_observations(self, analysis: Dict, attention_results: Dict) -> str:
        """ðŸ”¥ REAL: Describe what AI actually sees in THIS specific image"""
        observations = []
        
        # Analyze actual attention pattern
        attention_concentration = analysis.get('attention_concentration', 0)
        attention_symmetry = analysis.get('attention_symmetry', 0)
        
        if attention_concentration > 0.8:
            observations.append("â€¢ AI phÃ¡t hiá»‡n **má»™t vÃ¹ng cá»¥ thá»ƒ ráº¥t Ä‘Ã¡ng chÃº Ã½** trong áº£nh")
        elif attention_concentration > 0.5:
            observations.append("â€¢ AI nháº­n ra **vÃ i khu vá»±c quan trá»ng** cáº§n phÃ¢n tÃ­ch ká»¹")
        else:
            observations.append("â€¢ AI cáº§n **quÃ©t toÃ n bá»™ áº£nh** Ä‘á»ƒ Ä‘Æ°a ra káº¿t luáº­n")
            
        # Real material analysis based on actual image content
        shine_ratio = analysis.get('shine_ratio', 0)
        surface_roughness = analysis.get('surface_roughness', 0.5)
        
        if shine_ratio > 0.4:
            observations.append(f"â€¢ Bá» máº·t cÃ³ **{shine_ratio*100:.0f}% vÃ¹ng bÃ³ng** - cháº¥t liá»‡u da/plastic")
        elif shine_ratio > 0.15:
            observations.append(f"â€¢ CÃ³ **{shine_ratio*100:.0f}% vÃ¹ng nháº¹ bÃ³ng** - cháº¥t liá»‡u há»—n há»£p")
        else:
            observations.append("â€¢ Bá» máº·t **hoÃ n toÃ n matte** - váº£i cotton/canvas")
            
        # Real texture analysis
        texture_strength = analysis.get('texture_strength', 0)
        if texture_strength > 60:
            observations.append(f"â€¢ Texture cÃ³ **Ä‘á»™ tÆ°Æ¡ng pháº£n {texture_strength:.0f}** - tháº¥y rÃµ vÃ¢n da/sá»£i váº£i")
        elif texture_strength > 30:
            observations.append(f"â€¢ Texture **trung bÃ¬nh ({texture_strength:.0f})** - cÃ³ pattern nhÆ°ng má»‹n")
        else:
            observations.append(f"â€¢ Bá» máº·t **ráº¥t má»‹n ({texture_strength:.0f})** - gáº§n nhÆ° khÃ´ng cÃ³ texture")
        
        # Real color analysis based on actual dominant colors
        dominant_colors = analysis.get('dominant_colors', [])
        if len(dominant_colors) >= 3:
            r, g, b = dominant_colors[:3]
            if max(r, g, b) > 200:
                observations.append(f"â€¢ MÃ u sáº¯c **sÃ¡ng** (RGB: {r:.0f}, {g:.0f}, {b:.0f}) - tone mÃ u cao")
            elif max(r, g, b) < 100:
                observations.append(f"â€¢ MÃ u sáº¯c **tá»‘i** (RGB: {r:.0f}, {g:.0f}, {b:.0f}) - tone mÃ u tháº¥p")
            else:
                observations.append(f"â€¢ MÃ u sáº¯c **trung bÃ¬nh** (RGB: {r:.0f}, {g:.0f}, {b:.0f}) - tone cÃ¢n báº±ng")
        
        # Real edge analysis
        focused_edge_density = analysis.get('focused_edge_density', 0)
        global_edge_density = analysis.get('global_edge_density', 0)
        
        if focused_edge_density > global_edge_density * 1.5:
            observations.append(f"â€¢ VÃ¹ng chÃº Ã½ cÃ³ **nhiá»u chi tiáº¿t** ({focused_edge_density:.1f} vs {global_edge_density:.1f}) - logo/pattern rÃµ nÃ©t")
        elif focused_edge_density > 0:
            observations.append(f"â€¢ Chi tiáº¿t **vá»«a pháº£i** trong vÃ¹ng quan trá»ng ({focused_edge_density:.1f})")
        else:
            observations.append("â€¢ **Ãt chi tiáº¿t** trong vÃ¹ng AI quan tÃ¢m - bá» máº·t Ä‘á»“ng nháº¥t")
            
        # Real manufacturing analysis
        stitching_quality = analysis.get('stitching_quality', 0)
        if stitching_quality > 0.3:
            observations.append(f"â€¢ PhÃ¡t hiá»‡n **Ä‘Æ°á»ng khÃ¢u** vá»›i cháº¥t lÆ°á»£ng {stitching_quality:.2f}")
        
        logo_sharpness = analysis.get('logo_sharpness', 0)
        if logo_sharpness > 0.5:
            observations.append(f"â€¢ Logo/text cÃ³ **Ä‘á»™ sáº¯c nÃ©t {logo_sharpness:.2f}** - in áº¥n cháº¥t lÆ°á»£ng cao")
        elif logo_sharpness > 0.1:
            observations.append(f"â€¢ Logo/text **Ä‘á»™ sáº¯c nÃ©t {logo_sharpness:.2f}** - in áº¥n trung bÃ¬nh")
        
        return "\n".join(observations)

    def _analyze_material_quality(self, analysis: Dict) -> str:
        """ðŸ”¥ REAL: Analyze what we actually detect in THIS image"""
        material_analysis = []
        
        # Real uniformity measurement
        material_uniformity = analysis.get('material_uniformity', 0.5)
        surface_smoothness = analysis.get('surface_smoothness', 0.5)
        
        if material_uniformity > 0.85:
            material_analysis.append(f"â€¢ **Äá»™ Ä‘á»“ng Ä‘á»u {material_uniformity:.2f}** - cháº¥t liá»‡u ráº¥t nháº¥t quÃ¡n (chÃ­nh hÃ£ng)")
        elif material_uniformity > 0.65:
            material_analysis.append(f"â€¢ **Äá»™ Ä‘á»“ng Ä‘á»u {material_uniformity:.2f}** - cháº¥t liá»‡u á»•n Ä‘á»‹nh (cháº¥p nháº­n Ä‘Æ°á»£c)")
        else:
            material_analysis.append(f"â€¢ **Äá»™ Ä‘á»“ng Ä‘á»u {material_uniformity:.2f}** - cháº¥t liá»‡u khÃ´ng á»•n Ä‘á»‹nh (nghi ngá»)")
        
        # Real surface analysis
        if surface_smoothness > 0.8:
            material_analysis.append(f"â€¢ **Bá» máº·t má»‹n {surface_smoothness:.2f}** - gia cÃ´ng cao cáº¥p")
        elif surface_smoothness > 0.5:
            material_analysis.append(f"â€¢ **Bá» máº·t {surface_smoothness:.2f}** - gia cÃ´ng tiÃªu chuáº©n")
        else:
            material_analysis.append(f"â€¢ **Bá» máº·t thÃ´ {surface_smoothness:.2f}** - gia cÃ´ng kÃ©m hoáº·c váº­t liá»‡u ráº»")
        
        # Real stitching analysis (if detected)
        stitching_quality = analysis.get('stitching_quality', 0)
        if stitching_quality > 0.1:  # Only mention if actually detected
            if stitching_quality > 0.6:
                material_analysis.append(f"â€¢ **ÄÆ°á»ng may xuáº¥t sáº¯c** (chá»‰ sá»‘ {stitching_quality:.2f}) - thá»£ may chuyÃªn nghiá»‡p")
            elif stitching_quality > 0.3:
                material_analysis.append(f"â€¢ **ÄÆ°á»ng may tá»‘t** (chá»‰ sá»‘ {stitching_quality:.2f}) - tiÃªu chuáº©n cÃ´ng nghiá»‡p")
            else:
                material_analysis.append(f"â€¢ **ÄÆ°á»ng may yáº¿u** (chá»‰ sá»‘ {stitching_quality:.2f}) - cÃ³ thá»ƒ sáº£n xuáº¥t kÃ©m")
        
        # Real color bleeding analysis
        color_bleeding = analysis.get('color_bleeding', 0)
        if color_bleeding > 0:  # Only if detected
            if color_bleeding < 20:
                material_analysis.append(f"â€¢ **Ãt lem mÃ u** ({color_bleeding:.1f}) - nhuá»™m cháº¥t lÆ°á»£ng")
            elif color_bleeding < 40:
                material_analysis.append(f"â€¢ **Lem mÃ u vá»«a** ({color_bleeding:.1f}) - nhuá»™m bÃ¬nh thÆ°á»ng")
            else:
                material_analysis.append(f"â€¢ **Lem mÃ u nhiá»u** ({color_bleeding:.1f}) - nhuá»™m kÃ©m cháº¥t lÆ°á»£ng")
        
        # Pattern regularity (if detected)
        pattern_regularity = analysis.get('pattern_regularity', 0)
        if pattern_regularity > 0.1:  # Only if pattern exists
            if pattern_regularity > 0.7:
                material_analysis.append(f"â€¢ **Pattern ráº¥t Ä‘á»u** ({pattern_regularity:.2f}) - in cÃ´ng nghiá»‡p chÃ­nh xÃ¡c")
            elif pattern_regularity > 0.4:
                material_analysis.append(f"â€¢ **Pattern tÆ°Æ¡ng Ä‘á»‘i Ä‘á»u** ({pattern_regularity:.2f}) - in tiÃªu chuáº©n")
            else:
                material_analysis.append(f"â€¢ **Pattern khÃ´ng Ä‘á»u** ({pattern_regularity:.2f}) - in thá»§ cÃ´ng hoáº·c lá»—i")
        
        # Real contrast analysis
        global_contrast = analysis.get('global_contrast', 0)
        focused_contrast = analysis.get('focused_contrast', 0)
        
        if global_contrast > 0:
            if global_contrast > 50:
                material_analysis.append(f"â€¢ **Äá»™ tÆ°Æ¡ng pháº£n cao** ({global_contrast:.0f}) - chi tiáº¿t rÃµ nÃ©t")
            elif global_contrast > 25:
                material_analysis.append(f"â€¢ **Äá»™ tÆ°Æ¡ng pháº£n vá»«a** ({global_contrast:.0f}) - chi tiáº¿t bÃ¬nh thÆ°á»ng")
            else:
                material_analysis.append(f"â€¢ **Äá»™ tÆ°Æ¡ng pháº£n tháº¥p** ({global_contrast:.0f}) - áº£nh má» hoáº·c Ã¡nh sÃ¡ng yáº¿u")
        
        return "\n".join(material_analysis) if material_analysis else "â€¢ KhÃ´ng phÃ¡t hiá»‡n Ä‘áº·c Ä‘iá»ƒm cháº¥t liá»‡u rÃµ rÃ ng trong áº£nh nÃ y"

    def _identify_authenticity_markers(self, analysis: Dict, is_fake: bool) -> str:
        """ðŸ”¥ REAL: Identify specific markers based on actual measurements"""
        markers = []
        
        # Real geometric accuracy based on measurements
        shape_regularity = analysis.get('shape_regularity', 0.5)
        alignment_score = analysis.get('alignment_score', 0.5)
        
        if shape_regularity > 0 and alignment_score > 0:  # Only if we actually measured
            combined_score = (shape_regularity + alignment_score) / 2
            if combined_score > 0.85:
                markers.append(f"â€¢ **Geometry score {combined_score:.2f}** - {'quÃ¡ hoÃ n háº£o (nghi ngá» copy)' if is_fake else 'chÃ­nh xÃ¡c cao (authentic)'}")
            elif combined_score > 0.6:
                markers.append(f"â€¢ **Geometry score {combined_score:.2f}** - {'tá»‘t nhÆ°ng cÃ³ lá»—i nhá»' if is_fake else 'tiÃªu chuáº©n cÃ´ng nghiá»‡p'}")
            else:
                markers.append(f"â€¢ **Geometry score {combined_score:.2f}** - {'sáº£n xuáº¥t kÃ©m' if is_fake else 'handmade/vintage'}")
        
        # Real pattern analysis (only if pattern detected)
        pattern_regularity = analysis.get('pattern_regularity', 0)
        if pattern_regularity > 0.1:
            if pattern_regularity > 1.2:
                markers.append(f"â€¢ **Pattern regularity {pattern_regularity:.2f}** - {'copy ká»¹ thuáº­t sá»‘' if is_fake else 'mÃ¡y in hiá»‡n Ä‘áº¡i'}")
            elif pattern_regularity > 0.7:
                markers.append(f"â€¢ **Pattern regularity {pattern_regularity:.2f}** - {'cháº¥t lÆ°á»£ng tá»‘t' if not is_fake else 'fake cháº¥t lÆ°á»£ng cao'}")
            else:
                markers.append(f"â€¢ **Pattern irregularity {pattern_regularity:.2f}** - {'in lá»—i' if is_fake else 'handprinted'}")
        
        # Real logo analysis (only if logo detected)
        logo_sharpness = analysis.get('logo_sharpness', 0)
        focused_edge_density = analysis.get('focused_edge_density', 0)
        
        if logo_sharpness > 0.1:  # Logo was actually detected
            if logo_sharpness > 0.8 and focused_edge_density > 50:
                markers.append(f"â€¢ **Logo clarity {logo_sharpness:.2f}** & **edge density {focused_edge_density:.0f}** - {'scan cháº¥t lÆ°á»£ng cao' if is_fake else 'emboss/laser chÃ­nh hÃ£ng'}")
            elif logo_sharpness > 0.5:
                markers.append(f"â€¢ **Logo clarity {logo_sharpness:.2f}** - {'in tá»‘t' if not is_fake else 'fake quality decent'}")
            else:
                markers.append(f"â€¢ **Logo blur {logo_sharpness:.2f}** - {'in kÃ©m' if is_fake else 'worn/vintage'}")
        
        # Real material consistency markers
        material_uniformity = analysis.get('material_uniformity', 0)
        surface_smoothness = analysis.get('surface_smoothness', 0)
        
        if material_uniformity > 0:
            material_score = (material_uniformity + surface_smoothness) / 2
            if material_score > 0.9:
                markers.append(f"â€¢ **Material score {material_score:.2f}** - {'cá»±c ká»³ Ä‘á»u (nghi ngá» synthetic)' if is_fake else 'cháº¥t lÆ°á»£ng cao cáº¥p'}")
            elif material_score > 0.7:
                markers.append(f"â€¢ **Material score {material_score:.2f}** - {'cháº¥t lÆ°á»£ng tá»‘t' if not is_fake else 'fake grade A'}")
            elif material_score > 0.4:
                markers.append(f"â€¢ **Material score {material_score:.2f}** - {'cháº¥t lÆ°á»£ng trung bÃ¬nh' if not is_fake else 'fake grade B'}")
            else:
                markers.append(f"â€¢ **Material score {material_score:.2f}** - {'cháº¥t lÆ°á»£ng kÃ©m' if is_fake else 'damaged/old'}")
        
        # Real color analysis
        color_vibrancy = analysis.get('color_vibrancy', 0)
        color_bleeding = analysis.get('color_bleeding', 0)
        
        if color_vibrancy > 0 and color_bleeding >= 0:
            if color_vibrancy > 0.8 and color_bleeding < 10:
                markers.append(f"â€¢ **Color perfect** (vibrancy {color_vibrancy:.2f}, bleeding {color_bleeding:.0f}) - {'nghi ngá» digital print' if is_fake else 'nhuá»™m chuyÃªn nghiá»‡p'}")
            elif color_bleeding > 40:
                markers.append(f"â€¢ **Color bleeding high** ({color_bleeding:.0f}) - {'dye cheap' if is_fake else 'natural aging'}")
        
        # Attention pattern analysis
        attention_concentration = analysis.get('attention_concentration', 0)
        if attention_concentration > 0.9:
            markers.append(f"â€¢ **AI focus extreme** ({attention_concentration:.2f}) - cÃ³ má»™t chi tiáº¿t ráº¥t Ä‘Ã¡ng ngá»")
        elif attention_concentration < 0.2:
            markers.append(f"â€¢ **AI scan toÃ n diá»‡n** ({attention_concentration:.2f}) - khÃ´ng cÃ³ Ä‘iá»ƒm báº¥t thÆ°á»ng")
        
        return "\n".join(markers) if markers else "â€¢ KhÃ´ng phÃ¡t hiá»‡n dáº¥u hiá»‡u Ä‘áº·c biá»‡t nÃ o trong áº£nh nÃ y"
        
        # Material consistency
        num_colors = analysis.get('num_dominant_colors', 0)
        if num_colors > 8:
            markers.append("â€¢ **QuÃ¡ nhiá»u mÃ u sáº¯c** - cÃ³ thá»ƒ lÃ  photo composite")
        elif num_colors < 2:
            markers.append("â€¢ **QuÃ¡ Ã­t mÃ u sáº¯c** - cÃ³ thá»ƒ thiáº¿u detail")
        
        # Surface texture authenticity
        texture_directionality = analysis.get('texture_directionality', 0)
        if texture_directionality > 2.5:
            markers.append("â€¢ **Texture quÃ¡ random** - cÃ³ thá»ƒ lÃ  fake texture overlay")
        elif texture_directionality < 0.5:
            markers.append("â€¢ **Texture Ä‘á»“ng nháº¥t** - sáº£n xuáº¥t industrial")
        
        return "\n".join(markers) if markers else "â€¢ KhÃ´ng phÃ¡t hiá»‡n dáº¥u hiá»‡u Ä‘áº·c biá»‡t rÃµ rÃ ng"

    def _explain_ai_reasoning(self, analysis: Dict, prediction: str, confidence: float, attention_results: Dict) -> str:
        """Explain the AI's reasoning process"""
        reasoning = []
        
        # Attention method used
        methods_used = list(attention_results.keys())
        if 'gradient' in methods_used:
            reasoning.append("â€¢ Sá»­ dá»¥ng **Gradient Analysis** - phÆ°Æ¡ng phÃ¡p chÃ­nh xÃ¡c nháº¥t, theo dÃµi gradient flows")
        if 'fused' in methods_used:
            reasoning.append("â€¢ Ãp dá»¥ng **Multi-method Fusion** - káº¿t há»£p nhiá»u gÃ³c Ä‘á»™ phÃ¢n tÃ­ch")
        
        # Decision factors
        high_attention = analysis.get('high_attention_area', 0)
        concentration = analysis.get('attention_concentration', 0.5)
        
        if concentration > 0.7:
            reasoning.append(f"â€¢ AI **táº­p trung cao** ({concentration:.1%}) vÃ o vÃ¹ng quan trá»ng - quyáº¿t Ä‘á»‹nh dá»±a trÃªn chi tiáº¿t cá»¥ thá»ƒ")
        elif concentration < 0.3:
            reasoning.append(f"â€¢ AI **phÃ¢n tÃ¡n attention** ({concentration:.1%}) - Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ toÃ n áº£nh")
        
        # Confidence reasoning
        if confidence > 0.8:
            reasoning.append("â€¢ **CÃ¡c chá»‰ sá»‘ Ä‘á»“ng thuáº­n** - nhiá»u feature cÃ¹ng hÆ°á»›ng káº¿t luáº­n")
        elif confidence < 0.6:
            reasoning.append("â€¢ **TÃ­n hiá»‡u mÃ¢u thuáº«n** - má»™t sá»‘ feature khÃ´ng nháº¥t quÃ¡n")
        
        # Specific decision drivers
        material_score = analysis.get('material_uniformity', 0.5) * analysis.get('surface_smoothness', 0.5)
        logo_score = analysis.get('logo_sharpness', 0.5)
        
        if material_score > 0.6 and logo_score > 0.6:
            reasoning.append("â€¢ **Cháº¥t lÆ°á»£ng material + logo tá»‘t** â†’ há»— trá»£ authentic")
        elif material_score < 0.4 or logo_score < 0.3:
            reasoning.append("â€¢ **Cháº¥t lÆ°á»£ng material hoáº·c logo kÃ©m** â†’ nghi ngá» fake")
        
        return "\n".join(reasoning)

    def _break_down_confidence(self, analysis: Dict, confidence: float) -> str:
        """Break down confidence into contributing factors"""
        factors = []
        
        # Material quality contribution
        material_uniformity = analysis.get('material_uniformity', 0.5)
        if material_uniformity > 0.8:
            factors.append("â€¢ **Cháº¥t liá»‡u Ä‘á»“ng Ä‘á»u** (+15% confidence)")
        elif material_uniformity < 0.4:
            factors.append("â€¢ **Cháº¥t liá»‡u khÃ´ng Ä‘á»u** (-10% confidence)")
        
        # Logo/text quality
        logo_sharpness = analysis.get('logo_sharpness', 0.5)
        if logo_sharpness > 0.7:
            factors.append("â€¢ **Logo sáº¯c nÃ©t** (+10% confidence)")
        elif logo_sharpness < 0.3:
            factors.append("â€¢ **Logo má»** (-15% confidence)")
        
        # Color vibrancy
        color_vibrancy = analysis.get('color_vibrancy', 0.5)
        if color_vibrancy > 0.7:
            factors.append("â€¢ **MÃ u sáº¯c tá»± nhiÃªn** (+5% confidence)")
        elif color_vibrancy < 0.3:
            factors.append("â€¢ **MÃ u sáº¯c nháº¡t** (-5% confidence)")
        
        # Attention pattern
        concentration = analysis.get('attention_concentration', 0.5)
        if concentration > 0.7:
            factors.append("â€¢ **Focus rÃµ rÃ ng** (+8% confidence)")
        elif concentration < 0.3:
            factors.append("â€¢ **KhÃ´ng focus** (-8% confidence)")
        
        # Surface texture
        texture_strength = analysis.get('texture_strength', 0.5)
        if texture_strength > 0.8:
            factors.append("â€¢ **Texture chi tiáº¿t** (+7% confidence)")
        elif texture_strength < 0.2:
            factors.append("â€¢ **Texture thiáº¿u** (-10% confidence)")
        
        if confidence > 0.8:
            factors.append(f"\n**Káº¿t luáº­n**: Nhiá»u yáº¿u tá»‘ tÃ­ch cá»±c â†’ **{confidence:.1%} confidence**")
        elif confidence < 0.6:
            factors.append(f"\n**Káº¿t luáº­n**: CÃ¡c yáº¿u tá»‘ mÃ¢u thuáº«n â†’ **{confidence:.1%} confidence**")
        else:
            factors.append(f"\n**Káº¿t luáº­n**: CÃ¢n báº±ng giá»¯a cÃ¡c yáº¿u tá»‘ â†’ **{confidence:.1%} confidence**")
        
        return "\n".join(factors)

    def visualize_explanation(self, original_image: np.ndarray, result: Dict, save_path: Optional[str] = None) -> plt.Figure:
        """ðŸ”¥ ENHANCED: Create comprehensive and beautiful visualization"""
        
        # Determine layout based on available heatmaps
        num_heatmaps = len(result.get('all_heatmaps', {'main': result['heatmap']}))
        
        if num_heatmaps <= 3:
            fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        else:
            fig, axes = plt.subplots(3, 3, figsize=(24, 24))
            
        fig.suptitle(f"ðŸ”¥ ENHANCED AI Analysis: {result['prediction']} ({result['confidence']:.1%})", 
                     fontsize=24, weight='bold', color='darkblue')

        # Flatten axes for easier indexing
        axes_flat = axes.flatten()
        plot_idx = 0

        # 1. Original Image
        axes_flat[plot_idx].imshow(original_image)
        axes_flat[plot_idx].set_title('ðŸ“¸ Original Image', fontsize=16, weight='bold')
        axes_flat[plot_idx].axis('off')
        plot_idx += 1

        # 2. Primary Enhanced Heatmap
        primary_heatmap = result['heatmap']
        im1 = axes_flat[plot_idx].imshow(primary_heatmap, cmap=self.cmap_heat)
        axes_flat[plot_idx].set_title('ðŸ”¥ Enhanced Attention Heatmap', fontsize=16, weight='bold')
        axes_flat[plot_idx].axis('off')
        fig.colorbar(im1, ax=axes_flat[plot_idx], shrink=0.8, label='Attention Intensity')
        plot_idx += 1

        # 3. Attention Overlay
        try:
            overlay = original_image.astype(float) / 255.0
            heatmap_colored = self.cmap_heat(primary_heatmap)[:, :, :3]
            blended = 0.65 * overlay + 0.35 * heatmap_colored
            axes_flat[plot_idx].imshow(blended)
        except Exception as e:
            axes_flat[plot_idx].imshow(original_image)
            
        axes_flat[plot_idx].set_title('ðŸŽ¯ Attention Overlay', fontsize=16, weight='bold')
        axes_flat[plot_idx].axis('off')
        plot_idx += 1

        # 4-6. Multiple Heatmap Types (if available)
        all_heatmaps = result.get('all_heatmaps', {})
        heatmap_titles = {
            'gradient': 'ðŸ§  Gradient-Based Attention',
            'weights': 'âš–ï¸ Attention Weights',
            'activation': 'ðŸ’« Activation-Based',
            'fused': 'ðŸ”€ Multi-Method Fusion',
            'enhanced': 'âœ¨ Enhanced Processing'
        }
        
        colormaps = [self.cmap_heat, self.cmap_cool, self.cmap_focus, 'viridis', 'plasma']
        
        for i, (hmap_type, hmap_data) in enumerate(all_heatmaps.items()):
            if plot_idx >= len(axes_flat) - 1 or hmap_type == 'enhanced':  # Skip enhanced as it's already shown
                continue
                
            if hmap_data is not None:
                cmap_to_use = colormaps[i % len(colormaps)]
                im = axes_flat[plot_idx].imshow(hmap_data, cmap=cmap_to_use)
                title = heatmap_titles.get(hmap_type, f'{hmap_type.title()} Attention')
                axes_flat[plot_idx].set_title(title, fontsize=14, weight='bold')
                axes_flat[plot_idx].axis('off')
                fig.colorbar(im, ax=axes_flat[plot_idx], shrink=0.6)
                plot_idx += 1

        # 7. Attention Statistics Visualization
        if plot_idx < len(axes_flat):
            self._plot_attention_statistics(axes_flat[plot_idx], result['content_analysis'])
            plot_idx += 1

        # 8. Focus Region Analysis
        if plot_idx < len(axes_flat):
            self._plot_focus_regions(axes_flat[plot_idx], original_image, primary_heatmap)
            plot_idx += 1

        # Hide remaining empty subplots
        for i in range(plot_idx, len(axes_flat)):
            axes_flat[i].axis('off')

        # Add comprehensive explanation text
        explanation_text = result['explanation']
        # Truncate if too long for display
        if len(explanation_text) > 800:
            explanation_text = explanation_text[:800] + "..."
            
        plt.figtext(0.5, 0.02, explanation_text, ha="center", fontsize=11, 
                   wrap=True, bbox={"facecolor": "lightblue", "alpha": 0.7, "pad": 10})
        
        plt.tight_layout(rect=[0, 0.15, 1, 0.95])

        if save_path:
            ensure_dir(os.path.dirname(save_path))
            plt.savefig(save_path, dpi=200, bbox_inches='tight', facecolor='white')
            print(f"ðŸ’¾ Enhanced visualization saved to {save_path}")

        return fig

    def _plot_attention_statistics(self, ax, analysis: Dict):
        """Plot attention pattern statistics"""
        stats_to_plot = []
        values = []
        
        if 'attention_concentration' in analysis:
            stats_to_plot.extend(['Concentration', 'Symmetry', 'Dispersion'])
            values.extend([
                analysis['attention_concentration'],
                analysis['attention_symmetry'], 
                analysis['attention_dispersion']
            ])
        
        if 'high_attention_area' in analysis:
            stats_to_plot.extend(['High Focus %', 'Medium Focus %'])
            values.extend([
                analysis['high_attention_area'] / 100,
                analysis['medium_attention_area'] / 100
            ])
        
        if stats_to_plot and values:
            colors = plt.cm.Set3(np.linspace(0, 1, len(stats_to_plot)))
            bars = ax.bar(stats_to_plot, values, color=colors)
            ax.set_title('ðŸ“Š Attention Pattern Statistics', fontsize=14, weight='bold')
            ax.set_ylim(0, 1)
            ax.set_ylabel('Value')
            
            # Add value labels on bars
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.2f}', ha='center', va='bottom', fontsize=10)
        else:
            ax.text(0.5, 0.5, 'No Statistics\nAvailable', ha='center', va='center',
                   transform=ax.transAxes, fontsize=14)
            ax.set_title('ðŸ“Š Attention Statistics', fontsize=14, weight='bold')

    def _plot_focus_regions(self, ax, image: np.ndarray, heatmap: np.ndarray):
        """Visualize focus regions with contours"""
        try:
            # Create focus contours
            high_thresh = np.percentile(heatmap, 90)
            medium_thresh = np.percentile(heatmap, 70)
            
            high_mask = (heatmap > high_thresh).astype(np.uint8)
            medium_mask = (heatmap > medium_thresh).astype(np.uint8)
            
            # Show original image
            ax.imshow(image)
            
            # Add contours
            if np.any(high_mask):
                high_contours, _ = cv2.findContours(high_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                for contour in high_contours:
                    contour = contour.squeeze()
                    if len(contour.shape) == 2 and contour.shape[0] > 2:
                        ax.plot(contour[:, 0], contour[:, 1], 'r-', linewidth=3, label='High Focus')
            
            if np.any(medium_mask):
                medium_contours, _ = cv2.findContours(medium_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                for contour in medium_contours:
                    contour = contour.squeeze()
                    if len(contour.shape) == 2 and contour.shape[0] > 2:
                        ax.plot(contour[:, 0], contour[:, 1], 'y--', linewidth=2, label='Medium Focus')
            
            ax.set_title('ðŸŽ¯ Focus Region Analysis', fontsize=14, weight='bold')
            ax.axis('off')
            
            # Add legend
            handles, labels = ax.get_legend_handles_labels()
            if handles:
                ax.legend(handles[:2], labels[:2], loc='upper right')  # Avoid duplicate labels
                
        except Exception as e:
            ax.text(0.5, 0.5, f'Focus Analysis\nError: {str(e)[:30]}...', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=12)
            ax.set_title('ðŸŽ¯ Focus Region Analysis', fontsize=14, weight='bold')

def ensure_dir(directory: str):
    if directory and not os.path.exists(directory):
        os.makedirs(directory)


def generate_image_metrics(image: np.ndarray) -> Dict:
    """PhÃ¢n tÃ­ch cÃ¡c chá»‰ sá»‘ hÃ¬nh áº£nh cÆ¡ báº£n phá»¥c vá»¥ giáº£i thÃ­ch"""
    from scipy import ndimage

    gray = np.mean(image, axis=2).astype(np.float32)

    # Má»©c Ä‘á»™ sáº¯c nÃ©t (Sharpness)
    sobel_x = ndimage.sobel(gray, axis=0)
    sobel_y = ndimage.sobel(gray, axis=1)
    sharpness = np.mean(np.sqrt(sobel_x**2 + sobel_y**2)) / 255.0

    # Äá»™ Ä‘á»‘i xá»©ng (Symmetry) theo chiá»u ngang
    flipped = np.fliplr(gray)
    symmetry = 1.0 - np.mean(np.abs(gray - flipped)) / 255.0

    # Má»©c Ä‘á»™ chi tiáº¿t (Texture)
    laplacian = ndimage.laplace(gray)
    texture = np.std(laplacian) / 255.0

    # TÆ°Æ¡ng pháº£n viá»n (Edge precision)
    edge = ndimage.sobel(gray)
    edge_precision = np.mean(np.abs(edge)) / 255.0

    return {
        "sharpness": round(sharpness, 3),
        "symmetry": round(symmetry, 3),
        "texture": round(texture, 3),
        "edge_precision": round(edge_precision, 3)
    }


def summarize_verdict(confidence: float) -> str:
    """TÃ³m táº¯t káº¿t luáº­n cuá»‘i"""
    if confidence > 0.9:
        return "âœ”ï¸ CÃ³ kháº£ nÄƒng ráº¥t cao Ä‘Ã¢y lÃ  hÃ ng CHÃNH HÃƒNG"
    elif confidence > 0.7:
        return "âš ï¸ CÃ³ váº» lÃ  chÃ­nh hÃ£ng, nhÆ°ng cáº§n kiá»ƒm tra thÃªm"
    else:
        return "âŒ CÃ³ kháº£ nÄƒng lÃ  hÃ ng FAKE hoáº·c khÃ´ng Ä‘á»§ cÆ¡ sá»Ÿ káº¿t luáº­n"


def generate_ai_analysis(metrics: Dict, confidence: float) -> str:
    """Sinh lá»i phÃ¢n tÃ­ch dá»±a trÃªn chá»‰ sá»‘ tháº­t"""
    text = []
    text.append("ðŸ” **ÄÃNH GIÃ AI Dá»°A TRÃŠN HÃŒNH áº¢NH**")

    sharpness = metrics["sharpness"]
    symmetry = metrics["symmetry"]
    texture = metrics["texture"]
    edge = metrics["edge_precision"]

    # Sáº¯c nÃ©t
    if sharpness > 0.6:
        text.append("- HÃ¬nh áº£nh cÃ³ Ä‘á»™ sáº¯c nÃ©t cao, chi tiáº¿t hiá»ƒn thá»‹ rÃµ rÃ ng.")
    elif sharpness > 0.4:
        text.append("- Má»©c Ä‘á»™ sáº¯c nÃ©t tÆ°Æ¡ng Ä‘á»‘i á»•n Ä‘á»‹nh.")
    else:
        text.append("- HÃ¬nh áº£nh khÃ¡ má», thiáº¿u chi tiáº¿t ná»•i báº­t.")

    # Texture
    if texture > 0.08:
        text.append("- Bá» máº·t sáº£n pháº©m cÃ³ Ä‘á»™ texture phá»©c táº¡p, giá»‘ng Ä‘áº·c trÆ°ng hÃ ng tháº­t.")
    elif texture > 0.04:
        text.append("- Texture á»Ÿ má»©c trung bÃ¬nh, khÃ³ phÃ¢n biá»‡t rÃµ.")
    else:
        text.append("- Bá» máº·t má»‹n, texture Ä‘Æ¡n giáº£n â€” dáº¥u hiá»‡u cá»§a hÃ ng nhÃ¡i.")

    # Symmetry
    if symmetry > 0.85:
        text.append("- Sáº£n pháº©m Ä‘á»‘i xá»©ng cao, cho tháº¥y thiáº¿t káº¿ chuáº©n xÃ¡c.")
    elif symmetry > 0.7:
        text.append("- CÃ³ má»™t sá»‘ sai lá»‡ch Ä‘á»‘i xá»©ng nhá».")
    else:
        text.append("- Äá»‘i xá»©ng kÃ©m â€” kháº£ nÄƒng cao lÃ  lá»—i gia cÃ´ng hoáº·c báº£n copy.")

    # Edge precision
    if edge > 0.5:
        text.append("- CÃ¡c Ä‘Æ°á»ng nÃ©t rÃµ rÃ ng, viá»n sáº¯c â€” Ä‘iá»ƒm cá»™ng cho hÃ ng chuáº©n.")
    elif edge > 0.3:
        text.append("- Viá»n hÆ¡i má»m, chi tiáº¿t chÆ°a rÃµ nÃ©t.")
    else:
        text.append("- Viá»n má», chi tiáº¿t khÃ´ng rÃµ â€” cáº§n kiá»ƒm tra ká»¹ hÆ¡n.")

    # Káº¿t luáº­n cuá»‘i
    text.append(f"\nðŸ§  **Káº¿t luáº­n AI**: {summarize_verdict(confidence)}")

    return "\n".join(text)


def generate_heatmap(original_img: np.ndarray, cam: np.ndarray, save_path: str = "results/heatmap.jpg") -> str:
    """
    Ãp dá»¥ng Grad-CAM (cam) lÃªn áº£nh gá»‘c vÃ  lÆ°u heatmap káº¿t quáº£.
    - original_img: áº£nh RGB dáº¡ng ndarray
    - cam: máº£ng heatmap (giÃ¡ trá»‹ tá»« 0â€“1), thÆ°á»ng lÃ  output tá»« Grad-CAM
    - save_path: nÆ¡i lÆ°u heatmap.jpg
    """
    # Resize CAM vá» Ä‘Ãºng kÃ­ch thÆ°á»›c áº£nh gá»‘c
    cam_resized = cv2.resize(cam, (original_img.shape[1], original_img.shape[0]))
    cam_resized = np.uint8(255 * cam_resized)

    # Apply heatmap mÃ u
    heatmap = cv2.applyColorMap(cam_resized, cv2.COLORMAP_JET)

    # Convert áº£nh gá»‘c sang BGR Ä‘á»ƒ blend Ä‘Ãºng (vÃ¬ OpenCV dÃ¹ng BGR)
    original_bgr = cv2.cvtColor(original_img, cv2.COLOR_RGB2BGR)

    # Alpha blend giá»¯a áº£nh gá»‘c vÃ  heatmap
    blended = cv2.addWeighted(original_bgr, 0.6, heatmap, 0.4, 0)

    # LÆ°u heatmap
    cv2.imwrite(save_path, blended)

    return save_path